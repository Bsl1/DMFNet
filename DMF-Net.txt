import os
import io
import cv2
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import paddle
from paddle.nn import functional as F
import random
from paddle.io import  Dataset
from visualdl import LogWriter
from paddle.vision.transforms import transforms as T
# paddle.set_device('gpu')
import warnings
warnings.filterwarnings("ignore")

import paddle
import paddle.nn as nn
from paddle.vision.transforms import Compose, Resize
import matplotlib.pyplot as plt
from collections import OrderedDict
import copy
# import paddle.fluid as fluid
from paddle.vision.datasets import Cifar10
from paddle.vision.transforms import Normalize

import os
import random
from PIL import Image
import matplotlib.pyplot as plt

class Identity(nn.Layer):
    def __init__(self):
        super().__init__()
        
    def forward(self,x):
        return x

# 首先实现残差模块逻辑
class Block(nn.Layer):
    def __init__(self, in_dim, out_dim, stride):
        super().__init__()
        self.conv1 = nn.Conv2D(in_channels=in_dim, out_channels=out_dim, kernel_size=3, stride=stride, padding=1, bias_attr=False)
        self.b1 = nn.BatchNorm2D(out_dim)
        self.conv2 = nn.Conv2D(in_channels=out_dim, out_channels=out_dim, kernel_size=3, stride=1, padding=1, bias_attr=False)
        self.b2 = nn.BatchNorm2D(out_dim)
        self.relu = nn.ReLU()
        # 下采样判断
        if in_dim != out_dim or stride == 2:
            self.downsample = nn.Sequential(
                nn.Conv2D(in_channels=in_dim, out_channels=out_dim, kernel_size=3, stride=stride, padding=1, bias_attr=False),
                nn.BatchNorm2D(out_dim)
            )
        else:
            self.downsample = Identity()

    def forward(self, inputs):
        pre = inputs
        x = self.conv1(inputs)
        x = self.b1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.b2(x)
        identity = self.downsample(pre)
        x = x + identity
        out = self.relu(x)
        return out
class Resnet(nn.Layer):
    def __init__(self,
                 layers=18,
                 in_dim =3, 
                 out_dim = 64
                 ):
        
        super().__init__()
        self.dim = out_dim
        if layers == 18:
            self.bls=[2,2,2,2]
        elif layers == 34:
            self.bls=[3,4,6,3]
        else:
            raise Exception("参数layer只允许为18或者34, 其它层还没实现呢！")
        self.layer1 = self._make_layer(out_dim=64, blocks=self.bls[0], stride=1)
        self.layer2 = self._make_layer(out_dim=128, blocks=self.bls[1], stride=2)
        self.layer3 = self._make_layer(out_dim=256, blocks=self.bls[2], stride=2)
        self.layer4 = self._make_layer(out_dim=512, blocks=self.bls[3], stride=2)
        self.conv1 = nn.Conv2D(in_channels=in_dim, out_channels=out_dim, kernel_size=(7,7), stride=2, padding='same', bias_attr=False)
        self.relu = nn.ReLU()
        self.bn1 = nn.BatchNorm2D(out_dim)
    def forward(self,x):
        x=self.relu(self.bn1(self.conv1(x)))
        y1=self.layer1(x)
        y2=self.layer2(y1)
        y3=self.layer3(y2)
        y4=self.layer4(y3)
        return y1,y2,y3,y4
    def _make_layer(self, out_dim, blocks, stride):
        layerlist = []
        # 每个layer第一层的输入是上一层的输出
        layerlist.append(Block(in_dim=self.dim, out_dim=out_dim, stride=stride))
        # 其他block输入是本层的输出
        self.dim = out_dim
        for i in range(1, blocks):
            layerlist.append(Block(in_dim=self.dim, out_dim=out_dim, stride=1))
        
        return nn.Sequential(*layerlist)
class IAL(nn.Layer):
    def __init__(self, in_channels):
        super().__init__()
        self.conv1=sa_layer(in_channels)
        self.conv2=nn.Conv2D(2*in_channels,in_channels,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.relu=nn.ReLU()
        self.b1=nn.BatchNorm2D(in_channels)
        self.conv3=nn.Conv2D(in_channels,1,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.b2=nn.BatchNorm2D(1)
    def forward(self, x1, x2):
        x=x1+x2
        x=self.conv1(x)
        x=self.conv3(x)
        x=self.b2(x)
        x=self.relu(x)
        x1=x*x1
        x2=x*x2

        # x1=fluid.layers.elementwise_mul(x=x, y=x1, axis=0)
        # x2=fluid.layers.elementwise_mul(x=x, y=x2, axis=0)
        x=paddle.concat(x=[x1,x2],axis=1)
        x=self.conv2(x)
        x=self.b1(x)
        x=self.relu(x)
        return x
class sa_layer(nn.Layer):
    """Constructs a Channel Spatial Group module.
    Args:
        k_size: Adaptive selection of kernel size
    """

    def __init__(self, channel, groups=32):
        super(sa_layer, self).__init__()
        self.groups = groups
        self.avg_pool = nn.AdaptiveAvgPool2D(1)
        self.cweight = self.create_parameter(shape=[1, channel // (2 * groups), 1, 1],default_initializer=paddle.nn.initializer.Assign(paddle.zeros([1, channel // (2 * groups), 1, 1])))
        self.cbias = self.create_parameter(shape=[1, channel // (2 * groups), 1, 1],default_initializer=paddle.nn.initializer.Assign(paddle.ones([1, channel // (2 * groups), 1, 1])))
        self.sweight = self.create_parameter(shape=[1, channel // (2 * groups), 1, 1],default_initializer=paddle.nn.initializer.Assign(paddle.zeros([1, channel // (2 * groups), 1, 1])))
        self.sbias = self.create_parameter(shape=[1, channel // (2 * groups), 1, 1],default_initializer=paddle.nn.initializer.Assign(paddle.ones([1, channel // (2 * groups), 1, 1])))

        self.sigmoid = nn.Sigmoid()
        self.gn = nn.GroupNorm(channel // (2 * groups), channel // (2 * groups))

    @staticmethod
    def channel_shuffle(x, groups):
        b, c, h, w = x.shape

        x = paddle.reshape(x, [b, groups, -1, h, w])
        x = paddle.transpose(x, [0, 2, 1, 3, 4])

        # flatten
        x = paddle.reshape(x, [b, -1, h, w])

        return x

    def forward(self, x):
        b, c, h, w = x.shape

        x = paddle.reshape(x, [b * self.groups, -1, h, w])
        x_0, x_1 = paddle.chunk(x, 2, axis=1)

        # channel attention
        xn = self.avg_pool(x_0)
        xn = self.cweight * xn + self.cbias
        xn = x_0 * self.sigmoid(xn)

        # spatial attention
        xs = self.gn(x_1)
        xs = self.sweight * xs + self.sbias
        xs = x_1 * self.sigmoid(xs)

        # concatenate along channel axis
        out = paddle.concat([xn, xs], axis=1)
        out = paddle.reshape(out, [b, -1, h, w])

        out = self.channel_shuffle(out, 2)
        return out
class AFE(nn.Layer):
    def __init__(self, channels,in_channels,out_channels):
        super().__init__()
        self.conv1=nn.Conv2D(channels,64,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.conv2=nn.Conv2D(in_channels,out_channels,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.conv3=nn.Conv2D(out_channels,1,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.b1=nn.BatchNorm2D(64)
        self.b2=nn.BatchNorm2D(out_channels)
        
        self.mish=nn.Mish()
        self.sig=nn.Sigmoid()
    def forward(self, x1, x2):
        x1=self.conv1(x1)
        x1=self.b1(x1)
        x2=self.conv2(x2)
        x2=self.b2(x2)
        x=x1+x2
        x=self.mish(x)
        x=self.conv3(x)
        x=self.sig(x)
        x=x*x2
        x=x+x2
        return x
class Channel_Max_Pooling(nn.Layer):
    def __init__(self, kernel_size, stride):
        super().__init__()
        self.max_pooling = nn.MaxPool2D(
            kernel_size=kernel_size,
            stride=stride
        )
    def forward(self, x):
        x = x.transpose(perm=[0,3, 2, 1])  # (batch_size, w, h, chs)
        x = self.max_pooling(x)
        out = x.transpose(perm=[0,3, 2, 1])
        return out
class Channel_avg_Pooling(nn.Layer):
    def __init__(self, kernel_size, stride):
        super().__init__()
        self.avg_pooling = nn.AvgPool2D(
            kernel_size=kernel_size,
            stride=stride
        )
    def forward(self, x):
        x = x.transpose(perm=[0,3, 2, 1])  # (batch_size, w, h, chs)
        x = self.avg_pooling(x)
        out = x.transpose(perm=[0,3, 2, 1])
        return out
   
class TAM(nn.Layer):
    def __init__(self,in_channels,out_channels):
        super().__init__()
        self.maxpool=Channel_Max_Pooling((1, 128), (1, 128))
        self.avgpool=Channel_avg_Pooling((1, 128), (1, 128))
        self.conv1=nn.Conv2D(in_channels,out_channels,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.b1=nn.BatchNorm2D(out_channels)
        self.conv2=nn.Conv2D(3,1,kernel_size=(3,3), stride=1, padding='same', bias_attr=False)
        self.b2=nn.BatchNorm2D(1)
        self.sig=nn.Sigmoid()

    def forward(self,x):
        x1=self.maxpool(x)
        x2=self.avgpool(x)
        x3=self.conv1(x)
        x3=self.b1(x3)
        x4=paddle.concat(x=[x1,x2,x3],axis=1)
        x4=self.conv2(x4)
        x4=self.b2(x4)
        x4=self.sig(x4)
        x=x4*x
        return x
#transformer
def conv_1x1_bn(inp, oup):
    return nn.Sequential(
        nn.Conv2D(inp, oup, 1, 1, 0, bias_attr=False),
        nn.BatchNorm2D(oup),
        nn.Silu()
    )


def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):
    return nn.Sequential(
        nn.Conv2D(inp, oup, kernal_size, stride, 1, bias_attr=False),
        nn.BatchNorm2D(oup),
        nn.Silu()
    )

class PreNorm(nn.Layer):
    def __init__(self, axis, fn):
        super().__init__()
        self.norm = nn.LayerNorm(axis)
        self.fn = fn
    
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)


class FeedForward(nn.Layer):#MLP
    def __init__(self, axis, hidden_axis, dropout=0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(axis, hidden_axis),
            nn.Silu(),
            nn.Dropout(dropout),
            nn.Linear(hidden_axis, axis),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        return self.net(x)

class Attention(nn.Layer):
    def __init__(self, axis, heads=8, axis_head=64, dropout=0.):#axis_head:每个head的维度 。axis：输入transformer的每一个token的维度（序列长度）
        super().__init__()
        inner_axis = axis_head *  heads
        project_out = not (heads == 1 and axis_head == axis)

        self.heads = heads
        self.scale = axis_head ** -0.5

        self.attend = nn.Softmax(axis = -1)
        self.to_qkv = nn.Linear(axis, inner_axis * 3, bias_attr = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_axis, axis),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
 
        q,k,v = self.to_qkv(x).chunk(3, axis=-1)

        b,p,n,hd = q.shape
        b,p,n,hd = k.shape
        b,p,n,hd = v.shape
        q = q.reshape((b, p, n, self.heads, -1)).transpose((0, 1, 3, 2, 4))#unfold
        k = k.reshape((b, p, n, self.heads, -1)).transpose((0, 1, 3, 2, 4))#
        v = v.reshape((b, p, n, self.heads, -1)).transpose((0, 1, 3, 2, 4))#

        dots = paddle.matmul(q, k.transpose((0, 1, 2, 4, 3))) * self.scale
        attn = self.attend(dots)

        out = (attn.matmul(v)).transpose((0, 1, 3, 2, 4)).reshape((b, p, n,-1))
        return self.to_out(out)

class Transformer(nn.Layer):
    def __init__(self, axis, depth, heads, axis_head, mlp_axis, dropout=0.):
        super().__init__()
        self.layers = nn.LayerList([])
        for _ in range(depth):
            self.layers.append(nn.LayerList([
                PreNorm(axis, Attention(axis, heads, axis_head, dropout)),#先Norm层，再Self-attention
                PreNorm(axis, FeedForward(axis, mlp_axis, dropout))#先Norm，再MLP
            ]))
    
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x

class MobileViTBlock(nn.Layer):
    def __init__(self, axis, depth, channel, kernel_size, patch_size, mlp_axis, dropout=0.):#mlp_axis:MLP的输出节点数 axis:MLP的输入节点数（每个token的维度）
        super().__init__()
        self.ph, self.pw = patch_size

        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)
        self.conv2 = conv_1x1_bn(channel, axis)

        self.transformer = Transformer(axis, depth, 1, 32, mlp_axis, dropout)

        self.conv3 = conv_1x1_bn(axis, channel)
        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)


    def forward(self, x):
        y = x.clone()                      

        # Local representations
        x = self.conv1(x)
        x = self.conv2(x)
        # Global representations
        n, c, h, w = x.shape

        x = x.transpose((0,3,1,2)).reshape((n,self.ph * self.pw,-1,c))
        x = self.transformer(x)
        x = x.reshape((n,h,-1,c)).transpose((0,3,1,2))


        # Fusion
        x = self.conv3(x)
        x = paddle.concat((x, y), 1)
        x = self.conv4(x)
        return x



class Convblock1(nn.Layer):#MV前调通道数
    def __init__(self, ch_in, ch_out):
        super(Convblock1, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2D(ch_in, ch_out, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm(ch_out),
            nn.ReLU()
        ) 
    def forward(self, x):
        y = self.conv(x)
        return y

class transformer(nn.Layer):
    def __init__(self, image_size, axiss,channels, mlp_axis,channel1, kernel_size=3, patch_size=(2, 2)):#channels:是MVblock的输入和输出通道数 。取axiss为输入通道数channels的2/3倍。不知道为啥，MobileViT中都是2/3的倍数。
        super(transformer,self).__init__()
        ih, iw = image_size
        ph, pw = patch_size
        assert ih % ph == 0 and iw % pw == 0
        L = 2        
        self.mv1=MobileViTBlock(axiss, 2 , channels, kernel_size, patch_size,mlp_axis)#int(axiss*2)：MLP中全连接层的输出节点数。为4倍或2倍。浅层取2倍，深层取4倍。
        self.conv1=Convblock1(channel1, channels)
        self.pool=nn.MaxPool2D(kernel_size=2,stride=2)

    def forward(self, x):
        x = self.conv1(x)#调整进行MVblock的通道数
        y = self.mv1(x)
        y = self.pool(y)#MVblock输出后进行下采样
        return y

class up_conv(nn.Layer):
    def __init__(self, ch_in, ch_out):
        super(up_conv, self).__init__()
        self.up = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2D(ch_in, ch_out, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm(ch_out),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.up(x)
        return x


class CNN_Transformer_Net(nn.Layer):
    def __init__(self, img_ch=3, output_ch=2):
        super(CNN_Transformer_Net, self).__init__()
        #channel1：当前层的输入通道数，channels：当前层的输出通道数
        self.block21 = transformer(image_size=(256, 256), axiss =96, channels=64,channel1=3,patch_size=(8, 8),mlp_axis=96*2) 
        self.block22 = transformer(image_size=(128, 128), axiss =192, channels=128,channel1=64,patch_size=(8, 8),mlp_axis=192*2)
        self.block23 = transformer(image_size=(64, 64),   axiss =384, channels=256,channel1=128,patch_size=(2, 2),mlp_axis=384*2)
        self.block24 = transformer(image_size=(32, 32),   axiss =768, channels=512,channel1=256,patch_size=(2, 2),mlp_axis=768*2)
        self.resent=Resnet()
        self.upconv4=up_conv(ch_in=512,ch_out=256)#用于解码端进行上采样
        self.upconv3=up_conv(ch_in=256,ch_out=128)
        self.upconv2=up_conv(ch_in=128,ch_out=64)
        self.upconv1=up_conv(ch_in=64,ch_out=2)
        self.u1 = nn.Upsample(scale_factor=2, mode='bilinear')
        self.u2 = nn.Upsample(scale_factor=4, mode='bilinear')
        self.u3 = nn.Upsample(scale_factor=8, mode='bilinear')
        self.u4 = nn.Upsample(scale_factor=16, mode='bilinear')
        self.conv1=nn.Conv2D(960,64,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.conv2=nn.Conv2D(64,2,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.conv3=nn.Conv2D(64,2,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.conv4=nn.Conv2D(64,2,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.conv5=nn.Conv2D(64,2,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.conv6=nn.Conv2D(8,2,kernel_size=(1,1), stride=1, padding='same', bias_attr=False)
        self.IAL1=IAL(64)
        self.IAL2=IAL(128)
        self.IAL3=IAL(256)
        self.IAL4=IAL(512)
        self.AFE1=AFE(128,64,64)
        self.AFE2=AFE(256,128,64)
        self.AFE3=AFE(512,256,64)
        self.AFE4=AFE(64,512,64)
        self.AFE5=AFE(64,64,64)
        self.AFE6=AFE(64,64,64)
        self.AFE7=AFE(64,64,64)
        self.TAM1=AEE(64,1)
        self.TAM2=AEE(64,1)
        self.TAM3=AEE(64,1)
        self.TAM4=AEE(64,1)

    def forward(self, x):
        m1=self.resent(x)[0]# 64 128 128
        m2=self.resent(x)[1]#128 64  64
        m3=self.resent(x)[2]#256 32 32
        m4=self.resent(x)[3]# 512 16 16 
        t1 = self.block21(x)                         #64 128 128
        t2 = self.block22(t1)                        #128 64 64
        t3 = self.block23(t2)                        #256 32 32
        t4 = self.block24(t3)
        t1=self.IAL1(m1,t1)
        t2=self.IAL2(m2,t2)
        t3=self.IAL3(m3,t3)
        t4=self.IAL4(m4,t4)
        t1=self.u1(t1)#64
        t2=self.u2(t2)#128
        t3=self.u3(t3)#256
        t4=self.u4(t4)#512
        
        x=paddle.concat(x=[t1,t2,t3,t4],axis=1)               #512 16 16
        x=self.conv1(x)#64*16*16
       
        
        y1=self.AFE1(t2,t1)
        y1=self.AFE5(x,y1)
        y2=self.AFE2(t3,t2)
        y2=self.AFE6(x,y2)
        y3=self.AFE3(t4,t3)
        y3=self.AFE7(x,y3)
        y4=self.AFE4(x,t4)

        # y1=self.AFE1(x,t1)
        # y1=self.AFE5(y1,t2)
        # y2=self.AFE2(x,t2)
        # y2=self.AFE6(y2,t3)
        # y3=self.AFE3(x,t3)
        # y3=self.AFE7(y3,t4)
        # y4=self.AFE4(x,t4)

        y1=self.TAM1(y1)
        y2=self.TAM2(y2)
        y3=self.TAM3(y3)
        y4=self.TAM4(y4)

        y1=self.conv2(y1)
        y2=self.conv3(y2)
        y3=self.conv4(y3)
        y4=self.conv5(y4)
        x=paddle.concat(x=[y1,y2,y3,y4],axis=1)
        x=self.conv6(x)
        
        # up4 = self.upconv4(t4)
        # up3 = self.upconv3(up4)
        # up2 = self.upconv2(up3)
        # up1 =self.upconv1(up2)
        return y1,y2,y3,y4,x

IMAGE = (256, 256)
num_classes = 2
network = CNN_Transformer_Net(img_ch=3, output_ch=2)
model = paddle.Model(network)
model.summary((-1, 3) + IMAGE)
FLOPs = paddle.flops(network, [1, 3, 256, 256], custom_ops= None, print_detail=True)
print(FLOPs)